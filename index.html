
<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xianda Guo</title>
  
  <meta name="author" content="Xianda Guo">
  <meta name="viewport" content="width=device-width, initial-scale=1">  
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="images/png" href="imagess/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xianda Guo</name>
              </p>
               <p>
                 I am currently a Ph.D student at Wuhan University.
               </p>
               <p>
               I am interested in computer vision and autonomous driving. My current research focuses on:
               <li style="margin: 5px;" >
                 <b style="color:brown">Stereo Matching</b>
               </li>
                 <li style="margin: 5px;" >
                 <b style="color:brown">MLLM</b>
               </li>
               </p>
               <b>If you want to work with me (in person or remotely) as an intern, feel free to drop me an email at xianda_guo@163.com. I will support GPUs if we are a good fit.</b>
               <p style="text-align:center">
                 <a href="mailto:xianda_guo@163.com">Email</a> &nbsp/&nbsp
                 <a href="https://scholar.google.com/citations?user=jPvOqgYAAAAJ&hl=zh-CN"> Google Scholar</a> &nbsp/&nbsp
                 <a href="https://github.com/XiandaGuo"> GitHub </a>
               </p>
             </td>
             <td style="padding:2.5%;width:30%;max-width:30%">
               <img style="width:50%;max-width:50%" alt="profile photo" src="images/XiandaGuo.jpg">
             </td>
           </tr>
         </tbody></table>    
             

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	     <li style="margin: 5px;" >
                <b>2025-05:</b> One paper is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">T-MM</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-02:</b> One paper is accepted to <a href="https://mc.manuscriptcentral.com/tpami-cs">T-PAMI</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-02:</b> One paper is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">T-CSVT</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2025-01:</b> One paper is accepted to <a href="https://2025.ieee-icra.org/">ICRA 2025</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-07:</b> Two papers are accepted to <a href="https://eccv.ecva.net/Conferences/2024">ECCV 2024</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
      <td style="text-indent:20px;width:100%;vertical-align:middle">
      <p>
        *Equal contribution &nbsp;&nbsp; <sup>†</sup>Project leader/Corresponding author.
      </p>
      </td>
    </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="text-indent:20px;width:100%;vertical-align:middle">
          <p><heading>Newest Papers</heading></p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="images/StereoAnything.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>Stereo Anything: Unifying Stereo Matching with Large-Scale Mixed Data</papertitle>
          <br> 
          <strong>Xianda Guo*</strong>,
          <a >Chenming Zhang*</a>,
          <a >Youmin Zhang</a>,
          <a >Dujun Nie</a>,
          <a >Ruilin Wang</a>,
          <a href="https://wzzheng.net/">Wenzhao Zheng</a>,
          <a href="https://mattpoggi.github.io">Matteo Poggi</a>,
          <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=jzvXnkcAAAAJ">Long Chen</a>
          <br>
          <em><strong>arXiv</strong></em>, 2025.
          <br>
          <a href="https://arxiv.org/abs/2411.14053">[arXiv]</a>
          <a href="https://github.com/XiandaGuo/OpenStereo">[Code]</a>
          <br>
          <p> We introduce a novel synthetic dataset that complements existing data by adding variability in baselines, camera angles, and scene types. We extensively evaluate the zero-shot capabilities of our model on five public datasets, showcasing its impressive ability to generalize to new, unseen data. </p>
        </td>
      </tr>


    </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="text-indent:20px;width:100%;vertical-align:middle">
              <p><heading>Selected Papers <a href="https://scholar.google.com/citations?user=jPvOqgYAAAAJ&hl=zh-CN"> [Full List] </a></heading></p>
              <!-- <p>
                *Equal contribution &nbsp;&nbsp; <sup>†</sup>Project leader/Corresponding author.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          

          <h3 style="text-indent:20px;color:brown">Gait Recognition</h3>


           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Generalized_gait.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Exploring Generalized Gait Recognition: Reducing Redundancy and Noise within Indoor and Outdoor Datasets</papertitle>
              <br>
	      <a > Qian Zhou*</a>,
              <strong> Xianda Guo*<sup>†</sup></strong>,
              <a > Jilong Wang</a>,
              <a > Chuanfu Shen </a>,
              <a > Zhongyuan Wang </a>,
              <a > Hua Zou </a>,
              <a > Qin Zou </a>,
              <a > Chao Liang</a>,
              <a > Long Chen  </a>,
              <a > Gang Wu </a>
              <br>
              <em><strong>arXiv</strong></em>, 2025.
              <br>
              <a href="https://arxiv.org/pdf/2505.15176">[arXiv]</a>
              <a href="https://github.com/li1er3/Generalized_Gait">[Code]</a>
              <br>
              <p> We propose a unified framework for cross-domain gait recognition that mitigates dataset conflicts and data noise through separate triplet loss and targeted distillation, achieving robust generalization across diverse gait models. </p>
            </td>
          </tr>
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/sposgait.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Gait Recognition in the Wild: A Large-scale Benchmark and NAS-based Baseline</papertitle>
              <br>
              <strong>Xianda Guo*</strong>,
              <a > Zheng Zhu*</a>,
              <a > Tian Yang </a>,
              <a > Beibei Lin </a>,
              <a > Junjie Huang </a>,
              <a > Jiankang Deng </a>,
              <a > Guan Huang</a>,
              <a > Jiwen Lu </a>,
              <a > Jie Zhou </a>
              <br>
              <em><strong>T-PAMI</strong></em>, 2025.
              <br>
              <a href="https://arxiv.org/pdf/2205.02692">[arXiv]</a>
              <a href="https://github.com/XiandaGuo/SPOSGait?tab=readme-ov-file">[Code]</a>
              <br>
              <p>  The proposed GREW benchmark proves to be essential for both training and evaluating gait recognizers in unconstrained scenarios. In addition, we propose the Single Path One-Shot neural architecture search with uniform sampling for Gait recognition, named SPOSGait, which is the first NAS-based gait recognition model. </p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GaitC3I.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GaitC3I: Robust Cross-Covariate Gait Recognition via Causal Intervention</papertitle>
              <br>
              <a >  Jilong Wang </a>,
              <a > Saihui Hou </a>,
              <strong>Xianda Guo</strong>,
              <a > Yan Huang </a>,
              <a > Yongzhen Huang </a>,
              <a > Tianzhu Zhang </a>,
              <a > Liang Wang </a>
              <br>
              <em><strong>T-CSVT</strong></em>, 2025.
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10902484">[Paper]</a>
              <a >[Code]</a>
              <br>
              <p> We propose a Cross-Covariate Causal Intervention (GaitC3I) framework, a unified causality-inspired approach aimed at enhancing the robustness of gait recognition across diverse conditions. </p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DyGait.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DyGait: Exploiting Dynamic Representations for High-performance Gait Recognition</papertitle>
              <br>
              <a > Ming Wang* </a>,
              <strong>Xianda Guo*</strong>,
              <a > Beibei Lin </a>,
              <a > Tian Yang </a>,
              <a > Borui Zhang </a>,
              <a > Zheng Zhu</a>,
              <a > Lincheng Li</a>,
              <a > Shunli Zhang</a>,
              <a > Xin Yu</a>
              <br>
              <em><strong>ICCV</strong></em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2303.14953">[arXiv]</a>
              <a href="https://github.com/M-Candy77/DyGait">[Code]</a>
              <br>
              <p> We propose a novel and high-performance framework named DyGait. This is the first framework on gait recognition that is designed to focus on the extraction of dynamic features.</p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GaitRecognitionBench.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Gait Recognition in the Wild: A Benchmark</papertitle>
              <br> 
              <a >  Zheng Zhu* </a>,
              <strong>Xianda Guo*</strong>,
              <a > Tian Yang </a>,
              <a > Xin Tao </a>,  
              <a > Junjie Huang </a>,
              <a > Jiankang Deng </a>,
              <a > Guan Huang </a>,
              <a > Dalong Du </a>,
              <a > Jiwen Lu </a>,
              <a > Jie Zhou </a>
              <br>
              <em><strong>ICCV</strong></em>, 2021.
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Zhu_Gait_Recognition_in_the_Wild_A_Benchmark_ICCV_2021_paper.html">[Paper]</a>
              <a href="https://github.com/XiandaGuo/GREW-Benchmark">[Code]</a>
              <br>
              <p>  To the best of our knowledge, this is the first large-scale dataset for gait recognition in the wild. The proposed GREW benchmark proves to be essential for both training and evaluating gait recognizers in unconstrained scenarios. </p>
            </td>
          </tr>


          

          

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <h3 style="text-indent:20px;color:brown">🚙 Depth Estimation:</h3>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/MonoVIT.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer</papertitle>
              <br> 
              <a > Chaoqiang Zhao *</a>,
              <a >Youmin Zhang*</a>,
              <a > Matteo Poggi</a>,
              <a > Fabio Tosi</a>,
              <strong>Xianda Guo</strong>,
              <a >Tao Huang</a>,
              <a > Zheng Zhu</a>,
              <a > Guan Huang</a>,
              <a > Tian Yang </a>,
              <a >  Stefano Mattoccia </a>,
              <br>
              <em><strong>3DV</strong></em>, 2022.
              <br>
              <a href="https://arxiv.org/abs/2410.04417">[arXiv]</a> 
              <a href="https://github.com/zxcqlf/monovit">[Code]</a>
              <br>
              <p> In light of the recent successes achieved by Vision Transformers (ViTs), we propose MonoViT, a brand-new framework combining the global reasoning enabled by ViT models with the flexibility of self-supervised monocular depth estimation. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CompletionFormer.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>CompletionFormer: Depth Completion with Convolutions and Vision Transformers</papertitle>
              <br>
              <a >Youmin Zhang</a>,
              <strong>Xianda Guo</strong>,
              <a > Matteo Poggi</a>,
              <a > Zheng Zhu</a>,
              <a > Guan Huang</a>,
              <a > Stefano Mattoccia </a>,
              <br>
              <em><strong>CVPR</strong></em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2304.13030">[arXiv]</a>
              <a href="https://github.com/youmi-zym/CompletionFormer">[Code]</a>
              <br>
              <p>  This paper proposes a Joint Convolutional Attention and Transformer block (JCAT), which deeply couples the convolutional attention layer and Vision Transformer into one block, as the basic unit to construct our depth completion model in a pyramidal structure. </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DiffusionDepth.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation</papertitle>
              <br>
              <a >Yiqun Duan</a>,
              <strong>Xianda Guo<sup>†</sup></strong>,
              <a > Zheng Zhu</a>
              <br>
              <em><strong>ECCV</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2303.05021">[arXiv]</a>
              <a href="https://github.com/duanyiqun/DiffusionDepth">[Code]</a>
              <br>
              <p> We propose DiffusionDepth, a new approach that reformulates monocular depth estimation as a denoising diffusion process. It learns an iterative denoising process to `denoise' random depth distribution into a depth map with the guidance of monocular visual conditions. </p>
            </td>
          </tr>



        </tbody></table>

      


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
      
        <h3 style="text-indent:20px;color:green">🚙 Stereo Matching</h3>

	       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
      <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
          <img style="width:100%;max-width:100%" src="images/StereoAnything.png" alt="dise">
        </td>
        <td width="75%" valign="center">
          <papertitle>Stereo Anything: Unifying Stereo Matching with Large-Scale Mixed Data</papertitle>
          <br> 
          <strong>Xianda Guo*</strong>,
          <a >Chenming Zhang*</a>,
          <a href="https://youmi-zym.github.io/">Youmin Zhang</a>
          <a >Dujun Nie</a>,
          <a >Ruilin Wang</a>,
          <a href="https://wzzheng.net/">Wenzhao Zheng</a>,
	  <a href="https://mattpoggi.github.io">Matteo Poggi</a>,
          <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=jzvXnkcAAAAJ">Long Chen</a>
          <br>
          <em><strong>arXiv</strong></em>, 2025.
          <br>
          <a href="https://arxiv.org/abs/2411.14053">[arXiv]</a>
          <a href="https://github.com/XiandaGuo/OpenStereo">[Code]</a>
          <br>
          <p> We introduce a novel synthetic dataset that complements existing data by adding variability in baselines, camera angles, and scene types. We extensively evaluate the zero-shot capabilities of our model on five public datasets, showcasing its impressive ability to generalize to new, unseen data. </p>
        </td>
      </tr>
		       

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/OpenStereo.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong Baseline</papertitle>
            <br> 
            <strong>Xianda Guo</strong>,
            <a >Chenming Zhang</a>,
            <a > Juntao Lu</a>,
            <a > Yiqun Duan </a>,
            <a > Yiqi Wang </a>,
            <a > Tian Yang</a>,
            <a > Zheng Zhu</a>,
            <a > Long Chen</a>
            <br>
            <em><strong>arXiv</strong></em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2312.00343">[arXiv]</a>
            <a href="https://github.com/XiandaGuo/OpenStereo">[Code]</a>
            <br>
            <p> OpenStereo includes training and inference codes of more than 10 network models, making it, to our knowledge, the most complete stereo matching toolbox available. </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/LightStereo.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>LightStereo: Channel Boost Is All You Need for Efficient 2D Cost Aggregation</papertitle>
            <br> 
            <strong>Xianda Guo*</strong>,
            <a >Chenming Zhang*</a>,
            <a > Youmin Zhang </a>,
            <a >  Wenzhao Zheng</a>,
            <a > Dujun Nie </a>,
            <a > Matteo Poggi </a>,
            <a > Long Chen</a>
            <br>
            <em><strong>ICRA</strong></em>, 2025.
            <br>
            <a href="https://arxiv.org/abs/2406.19833">[arXiv]</a>
            <a href="https://github.com/XiandaGuo/OpenStereo">[Code]</a>
            <br>
            <p> We present LightStereo, a cutting-edge stereo-matching network crafted to accelerate the matching process.</p>
          </td>
        </tr>



      </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          
          
            <h3 style="text-indent:20px;color:green">🚙 End-to-End Driving</h3>
            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/MuskFuser.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle>MaskFuser: Masked Fusion of Joint Multi-Modal Tokenization for End-to-End Autonomous Driving</papertitle>
                <br>
                <a > Yiqun Duan</a>,
                <strong>Xianda Guo</strong>,
                <a > Zheng Zhu</a>,
                <a > Yao Zheng*</a>,
                <a > Zhen Wang</a>,
                <a > Yu-Kai Wang</a>,
                <a > Chin-Teng Lin</a>
                <br>
                <em><strong>arXiv</strong></em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2405.07573">[arXiv]</a>
                <a >[Code]</a>
                <br>
                <p> This paper proposes MaskFuser, which tokenizes various modalities into a unified semantic feature space and provides a joint representation for further behavior cloning in driving contexts. Given the unified token representation, MaskFuser is the first work to introduce cross-modality masked auto-encoder training. </p>
              </td>
            </tr>
          

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GenAD.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GenAD: Generative End-to-End Autonomous Driving</papertitle>
              <br> 
              <a>Wenzhao Zheng*</a>,
              <a href="https://github.com/songruiqi"> Ruiqi Song* </a>,  
              <strong> Xianda Guo*<sup>†</sup></strong>,
              <a > Chenming Zhang </a>, 
              <a href="https://scholar.google.com/citations?user=jzvXnkcAAAAJ"> Long Chen</a>
              <br>
              <em><strong>ECCV</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2402.11502">[arXiv]</a> 
              <a href="https://github.com/wzzheng/GenAD">[Code]</a>
              <br>
              <p> GenAD casts end-to-end autonomous driving as a generative modeling problem.  </p>
            </td>
          </tr>


        </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          
            <h3 style="text-indent:20px;color:green">🚙 LLM&MLLM</h3>



            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/InstructLLM.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle>Instruct Large Language Models to Drive like Humans</papertitle>
                <br>  
                <a > Ruijun Zhang*</a>,
                <strong>Xianda Guo*<sup>†</sup></strong>,
                <a > Wenzhao Zheng*</a>,
                <a > Chenming Zhang </a>,
                <a > Kurt Keutzer </a>,
                <a > Long Chen </a>
                <br>
                <em><strong>arXiv</strong></em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2406.07296">[arXiv]</a>
                <a href="https://github.com/bonbon-rj/InstructDriver">[Code]</a>

                <br>
                <p> In this paper, we propose an InstructDriver method to transform LLM into a motion planner with explicit instruction tuning to align its behavior with humans.</p>
              </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/drivemllm.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DriveMLLM: A Benchmark for Spatial Understanding with Multimodal Large Language Models in Autonomous Driving</papertitle>
              <br>
              <strong>Xianda Guo*</strong>,
              <a > Ruijun Zhang* </a>,
              <a > Yiqun Duan* </a>,
              <a > Yuhang He </a>,
              <a > Chenming Zhang </a>,
              <a > Shuai Liu</a>,
              <a > Long Chen</a>
              <br>
              <em><strong>arXiv</strong></em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2405.17429">[arXiv]</a> 
              <a href="https://github.com/XiandaGuo/Drive-MLLM">[Code]</a>
              <br>
              <p> We introduce DriveMLLM, a benchmark specifically designed to evaluate the spatial understanding capabilities of multimodal large language models (MLLMs) in autonomous driving.    </p>
            </td>
          </tr>

      </tbody></table>
      </div>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> ECCV 2024, ACM MM2025, NeurIPS2025
              </li>
              </li>
            </p>
	    <p>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-IP, T-MM, T-CSVT, RAL
              </li>
              </li>
            </p>
		  
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <!-- <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=3Xl5HqLv8wQcw477KsV8mPSQEnrm59hQ6peJ0jKbxdw&cl=ffffff&w=a"></script>
	  </div>         -->
	  <br>
	    &copy; Xianda Guo | Last updated: Mar. 1, 2025.
</center></p>
</body>

</html>
